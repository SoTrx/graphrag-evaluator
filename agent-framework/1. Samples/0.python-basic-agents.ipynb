{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2012ea09",
   "metadata": {},
   "source": [
    "# ü§ñ Basic Agents with Microsoft Agent Framework (Python)\n",
    "\n",
    "## üìã What You'll Learn\n",
    "\n",
    "This notebook shows you how to create AI agents using the Microsoft Agent Framework. You'll see three different ways to create agents and how to have conversations with them.\n",
    "\n",
    "**What's Inside:**\n",
    "- üèóÔ∏è **Three Agent Creation Methods**: GitHub Models, Azure OpenAI, and Azure AI Foundry\n",
    "- üõ†Ô∏è **Tool Integration**: Adding custom functions agents can call\n",
    "- üí¨ **Conversation Management**: Single-turn and multi-turn conversations\n",
    "- üîÑ **Streaming Responses**: Real-time response display\n",
    "\n",
    "## ‚öôÔ∏è Setup Requirements\n",
    "\n",
    "### 1. **Docker & Dev Container**\n",
    "- Install Docker on your machine\n",
    "- Open this project in the dev container (VS Code will prompt you)\n",
    "\n",
    "### 2. **Workspace Configuration**\n",
    "- Open the workspace: `File > Open Workspace from File` ‚Üí select `workspace.code-workspace`\n",
    "- This sets up the correct Python environment for each project folder\n",
    "\n",
    "### 3. **Environment Variables**\n",
    "1. Copy `.env.example` to `.env` in the `agent-framework-samples` folder:\n",
    "   ```bash\n",
    "   cp .env.example .env\n",
    "   ```\n",
    "2. Fill in your credentials:\n",
    "   ```env\n",
    "   # For GitHub Models (Option 1)\n",
    "   GITHUB_TOKEN=your_github_token\n",
    "   GITHUB_ENDPOINT=https://models.inference.ai.azure.com\n",
    "   GITHUB_MODEL_ID=gpt-4o-mini\n",
    "   \n",
    "   # For Azure OpenAI (Option 2)\n",
    "   AZURE_OPENAI_ENDPOINT=your_endpoint\n",
    "   AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=your_deployment\n",
    "   \n",
    "   # For Azure AI Foundry (Option 3)\n",
    "   AZURE_AI_PROJECT_ENDPOINT=your_project_endpoint\n",
    "   AZURE_AI_MODEL_DEPLOYMENT_NAME=your_model\n",
    "   \n",
    "   # For Local Ollama (Option 4)\n",
    "   OLLAMA_ENDPOINT=http://localhost:11434/v1\n",
    "   OLLAMA_DEPLOYMENT_NAME=llama3\n",
    "   ```\n",
    "\n",
    "## üöÄ What You'll Build\n",
    "\n",
    "Below you'll find code samples demonstrating:\n",
    "\n",
    "1. **Tool Definition**: A simple function that picks random travel destinations\n",
    "2. **Agent Configuration**: Instructions that define agent behavior\n",
    "3. **Three Creation Patterns**:\n",
    "   - **Option 1**: GitHub Models (simplest, uses OpenAI-compatible API)\n",
    "   - **Option 2**: Azure OpenAI (local agent, no persistence)\n",
    "   - **Option 3**: Azure AI Foundry (persistent agent in the cloud)\n",
    "4. **Conversation Examples**: Single-turn and multi-turn conversations with streaming\n",
    "\n",
    "Let's start building! üåü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df8a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os    \n",
    "import json                 \n",
    "from random import randint  \n",
    "\n",
    "from dotenv import load_dotenv  \n",
    "from agent_framework import ChatAgent\n",
    "from azure.identity import AzureCliCredential, DefaultAzureCredential\n",
    "from agent_framework.azure import AzureOpenAIChatClient\n",
    "from agent_framework.openai import OpenAIChatClient\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from agent_framework import ChatMessage, Role, ChatOptions\n",
    "from agent_framework.openai import OpenAIChatClient\n",
    "from azure.ai.projects.aio import AIProjectClient\n",
    "from agent_framework.azure import AzureAIAgentClient\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aef07d0",
   "metadata": {},
   "source": [
    "# Agents setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6507f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Tool Function Design Pattern\n",
    "# Common agent instructions definition \n",
    "def get_random_destination() -> str:\n",
    "    \"\"\"Get a random vacation destination using Repository Pattern.\n",
    "    \n",
    "    Returns:\n",
    "        str: A randomly selected destination following consistent format\n",
    "    \"\"\"\n",
    "    destinations = [\n",
    "        \"Barcelona, Spain\",      \n",
    "        \"Paris, France\",         \n",
    "        \"Berlin, Germany\",       \n",
    "        \"Tokyo, Japan\",          \n",
    "        \"Sydney, Australia\",     \n",
    "        \"New York, USA\",         \n",
    "        \"Cairo, Egypt\",          \n",
    "        \"Cape Town, South Africa\", \n",
    "        \"Rio de Janeiro, Brazil\",  \n",
    "        \"Bali, Indonesia\"          \n",
    "    ]\n",
    "    \n",
    "    return destinations[randint(0, len(destinations) - 1)]\n",
    "\n",
    "AGENT_NAME =\"TravelAgent\"\n",
    "\n",
    "AGENT_INSTRUCTIONS = \"\"\"You are a helpful AI Agent that can help plan vacations for customers.\n",
    "\n",
    "Important: When users specify a destination, always plan for that location. Only suggest random destinations when the user hasn't specified a preference.\n",
    "\n",
    "When the conversation begins, introduce yourself with this message:\n",
    "\"Hello! I'm your TravelAgent assistant. I can help plan vacations and suggest interesting destinations for you. Here are some things you can ask me:\n",
    "1. Plan a day trip to a specific location\n",
    "2. Suggest a random vacation destination\n",
    "3. Find destinations with specific features (beaches, mountains, historical sites, etc.)\n",
    "4. Plan an alternative trip if you don't like my first suggestion\n",
    "\n",
    "What kind of trip would you like me to help you plan today?\"\n",
    "\n",
    "Always prioritize user preferences. If they mention a specific destination like \"Bali\" or \"Paris,\" focus your planning on that location rather than suggesting alternatives.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da017ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Using OpenAI Chat Models via OpenAIChatClient\n",
    "\n",
    "chat_client = OpenAIChatClient(\n",
    "    base_url=os.environ.get(\"GITHUB_ENDPOINT\"), \n",
    "    api_key=os.environ.get(\"GITHUB_TOKEN\"),\n",
    "    model_id=os.environ.get(\"GITHUB_MODEL_ID\"))\n",
    "\n",
    "chat_agent = ChatAgent(\n",
    "        name = AGENT_NAME,\n",
    "        chat_client=chat_client,\n",
    "        instructions=AGENT_INSTRUCTIONS,\n",
    "        tools=[get_random_destination]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43272ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 2: Using Azure OpenAi client to generate a local agent\n",
    "\n",
    "chat_agent = AzureOpenAIChatClient(credential=DefaultAzureCredential()).create_agent(\n",
    "    instructions=AGENT_INSTRUCTIONS,\n",
    "    name = AGENT_NAME,\n",
    "    tools=[get_random_destination]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f7c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 3: Using persisted Azure AI Foundry Agents \n",
    "\n",
    "chat_agent : ChatAgent\n",
    "credential = DefaultAzureCredential()\n",
    "project_client = AIProjectClient(endpoint=os.environ[\"AZURE_AI_PROJECT_ENDPOINT\"],credential=credential)\n",
    "\n",
    "try:\n",
    "    # Create an agent that will persist\n",
    "    created_agent = await project_client.agents.create_agent(\n",
    "        model=os.environ[\"AZURE_AI_MODEL_DEPLOYMENT_NAME\"], \n",
    "        name=AGENT_NAME,\n",
    "        instructions=AGENT_INSTRUCTIONS\n",
    "    )\n",
    "    \n",
    "    chat_agent = ChatAgent(\n",
    "        chat_client=AzureAIAgentClient(project_client=project_client, agent_id=created_agent.id),\n",
    "        tools=[get_random_destination])\n",
    "    \n",
    "    print(f\"Agent {created_agent.name} created, and chat agent ready\")\n",
    "    \n",
    "finally:\n",
    "    print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95757d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 4 : Using OpenAI Chat Models via local Ollama OpenAPI-compatible endpoint\n",
    "\n",
    "chat_client = OpenAIChatClient(base_url= os.environ.get(\"OLLAMA_ENDPOINT\"),api_key=\"nokey\", model_id=os.environ.get(\"OLLAMA_DEPLOYMENT_NAME\"))\n",
    "\n",
    "chat_agent = ChatAgent(\n",
    "        chat_client=chat_client,\n",
    "        tools=[get_random_destination])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c58df3",
   "metadata": {},
   "source": [
    "# Agents Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new conversation instance with the agent storing the history of messages for this specific run\n",
    "thread = chat_agent.get_new_thread()\n",
    "\n",
    "response = await chat_agent.run(\"Plan me a day trip\",thread= thread)\n",
    "\n",
    "last_message = response.messages[-1]\n",
    "text_content = last_message.contents[0].text\n",
    "print(\"Travel plan:\")\n",
    "print(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3fe00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new user message to change the plan, on top of the past first round of conversation\n",
    "# Getting the answers as a stream : \n",
    "\n",
    "async for chunk in chat_agent.run_stream(\"I don't like that destination. Plan me another vacation.\", thread=thread):\n",
    "    if chunk.text:\n",
    "        print(chunk.text, end=\"\", flush=True)  # üìù Display response as it generates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0460b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new conversation instance with the agent and specify the strict response structure to follow\n",
    "\n",
    "class SubTask(BaseModel):\n",
    "    task_type: str = Field(\n",
    "        description=\"The specific agent assigned to handle this subtask\")\n",
    "    task_details: str = Field(\n",
    "        description=\"Detailed description of what needs to be done for this subtask\")\n",
    "\n",
    "\n",
    "class TravelPlan(BaseModel):\n",
    "    main_task: str = Field(\n",
    "        description=\"The overall travel request from the user\")\n",
    "    subtasks: List[SubTask] = Field(\n",
    "        description=\"List of subtasks broken down from the main task, each assigned to a specialized agent\")\n",
    "    \n",
    "# Create a new thread for this conversation\n",
    "thread = chat_agent.get_new_thread()\n",
    "\n",
    "# Run the agent with structured output format\n",
    "response = await chat_agent.run(\n",
    "    \"Plan me a day trip\",\n",
    "    thread=thread, \n",
    "    response_format=TravelPlan)\n",
    "\n",
    "# Parse the JSON response text directly into the Pydantic model\n",
    "travel_plan = TravelPlan.model_validate_json(response.text)\n",
    "\n",
    "# Display the structured travel plan\n",
    "print(\"‚úÖ Travel Plan:\")\n",
    "print(json.dumps(travel_plan.model_dump(), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-framework-samples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
